{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d0a763-3ff2-484a-bcfa-42972008982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b2aa4d-9803-4cd1-bc9f-c454746e7ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class NeuralAgent:\n",
    "    def __init__(self, alpha=0.5, epsilon=0.3, gamma=0.9):\n",
    "        self.alpha = alpha  # коэффициент обучения\n",
    "        self.epsilon = epsilon  # коэффициент исследования\n",
    "        self.gamma = gamma  # коэффициент дисконтирования\n",
    "        self.q_table = {}  # таблица Q-значений\n",
    "        self.model = self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim=9, activation='relu'))  # Входной слой с 64 нейронами\n",
    "        model.add(Dense(64, activation='relu'))  # Скрытый слой с 64 нейронами\n",
    "        model.add(Dense(9, activation='linear'))  # Выходной слой с 9 нейронами (по одному на каждое действие)\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')  # Компиляция модели\n",
    "        return model\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        return [i for i, value in enumerate(state) if value == 0]\n",
    "    \n",
    "    def choose_action(self, state, possible_actions):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            action = random.choice(possible_actions)  # случайное действие\n",
    "        else:\n",
    "            state = np.array([state])  # Преобразование состояния в двумерный numpy массив\n",
    "            q_values = self.model.predict(state)  # Получаем Q-значения из нейронной сети\n",
    "            q_values = q_values[0]\n",
    "            for i in range(len(q_values)):\n",
    "                if i not in possible_actions:\n",
    "                    q_values[i] = -np.inf  # Игнорируем недоступные действия\n",
    "            action = np.argmax(q_values)\n",
    "        return action\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        state = np.array([state])  # Преобразование состояния в двумерный numpy массив\n",
    "        next_state = np.array([next_state])  # Преобразование следующего состояния в двумерный numpy массив\n",
    "        target = reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "        q_values = self.model.predict(state)\n",
    "        q_values[0][action] = (1 - self.alpha) * q_values[0][action] + self.alpha * target\n",
    "        self.model.fit(state, q_values, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f51a594-e5db-4135-ac85-5e775d3b5d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(state, symbol):\n",
    "        winning_combinations = [[0, 1, 2], [1, 2, 0], [3, 4, 5], [4, 5, 3], [6, 7, 8], [7, 8, 6], [0, 2, 1], [3, 5, 4], [6, 8, 7], # горизонтальные\n",
    "                            [0, 3, 6], [3, 6, 0], [2, 5, 8], [5,8, 2], [1, 4, 7], [4, 7, 1], [0, 6, 3], [1, 7, 4], [2, 8, 5],  # вертикальные\n",
    "                            [0, 8, 4], [0, 4, 8], [4,8, 0], [2, 4, 6], [4, 6, 2], [2, 6, 4]]  # диагональные\n",
    "    \n",
    "        for combination in winning_combinations:\n",
    "            if state[combination[0]] == state[combination[1]] == symbol:\n",
    "                return { True: combination[2] }\n",
    "        return { False: combination[0] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2017fbad-5765-4b31-8bfe-c29d02da50c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultAgent:\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        return [i for i, value in enumerate(state) if value == 0]\n",
    "    \n",
    "    def choose_action(self, state, possible_actions):\n",
    "        result_check = check(state, -1)\n",
    "        for key in result_check:\n",
    "            if key:\n",
    "                action = result_check[key]\n",
    "            else:\n",
    "                action = random.choice(possible_actions)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45c48fe3-fff7-4ec4-a57e-343734a5bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(agent1, agent2):\n",
    "    state = [0, 0, 0, 0, 0, 0, 0, 0, 0]  # начальное состояние доски\n",
    "    game_over = False\n",
    "    \n",
    "    while not game_over:\n",
    "        # Ход первого агента\n",
    "        action1 = agent1.choose_action(tuple(state), agent1.get_possible_actions(state))\n",
    "        state[action1] = 1\n",
    "        \n",
    "        # Проверка на победу/ничью\n",
    "        if check_winner(state, 1):\n",
    "            rewards.append(1)\n",
    "            agent1.learn(tuple(state), action1, 1, tuple(state))\n",
    "            game_over = True\n",
    "            break\n",
    "\n",
    "        if not 0 in state:  # ничья\n",
    "            rewards.append(0)\n",
    "            agent1.learn(tuple(state), action1, 0, tuple(state))\n",
    "            game_over = True\n",
    "            break\n",
    "\n",
    "        # Ход второго агента\n",
    "        action2 = agent2.choose_action(tuple(state), agent1.get_possible_actions(state))\n",
    "        state[action2] = -1\n",
    "\n",
    "        # Проверка на победу/ничью\n",
    "        if check_winner(state, -1):\n",
    "            rewards.append(-1)\n",
    "            agent1.learn(tuple(state), action1, -1, tuple(state))\n",
    "            game_over = True\n",
    "            break\n",
    "\n",
    "    return game_over\n",
    "\n",
    "def check_winner(state, symbol):\n",
    "    winning_combinations = [[0, 1, 2], [3, 4, 5], [6, 7, 8],  # горизонтальные\n",
    "                            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # вертикальные\n",
    "                            [0, 4, 8], [2, 4, 6]]  # диагональные\n",
    "    \n",
    "    for combination in winning_combinations:\n",
    "        if state[combination[0]] == state[combination[1]] == state[combination[2]] == symbol:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc91054-2678-45d2-86ab-1ccf9f4ee3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_volume = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5189043b-0842-4f7d-ac43-21e395b43183",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = NeuralAgent()\n",
    "agent2 = DefaultAgent()\n",
    "\n",
    "total_games = 1000 * training_volume\n",
    "\n",
    "for i in range(total_games):\n",
    "    play_game(agent1, agent2)\n",
    "\n",
    "print(\"Studying complete!\")\n",
    "\n",
    "analysis_rewards = []\n",
    "\n",
    "counter = 0\n",
    "for index, data in enumerate(rewards):\n",
    "    if data == 1:\n",
    "        counter = counter + 1\n",
    "    if index % training_volume == 0:\n",
    "        analysis_rewards.append(counter)\n",
    "        counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887df79e-a689-449a-8503-da796a08e200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
